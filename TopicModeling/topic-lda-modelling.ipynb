{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "307822cd-8085-4570-883e-eb77ae60555e",
    "_uuid": "566fd8184c7bc038369b1d06267b991f879bc7e8"
   },
   "source": [
    "In this kernel I'll try some **Latent Dirichlet Allocation** to automaticallly extract the topics that charactereze Medium articles. Good tuning of LDA will give a really good result on the Leaderboard. If you are out of ideas try to add some LDA features to your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "d23cab33-054a-42ee-9cb6-0744f6d8ddc0",
    "_uuid": "e62f23fcdb347f63fd849f01a0e3cf2cb35932be"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy import sparse\n",
    "import pyLDAvis.gensim\n",
    "import gensim\n",
    "from gensim.matutils  import Sparse2Corpus\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06d79b8b-67c5-4871-a0e6-fd8b47e6bb46",
    "_uuid": "497b02dfbbd513a9472f03a6d2b96e2171fcb032"
   },
   "source": [
    "Let's start with standart preprocessing and get Bag Of Words from our text data with CountVectorizer. Preprocessing part was taken from the [kernel of Yury Kashnitsky](http://www.kaggle.com/kashnitsky/ridge-countvectorizer-baseline) with some modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3ed51b01-1a22-4fec-80aa-f216eacc04f1",
    "_uuid": "47e51c073bc060342d1a397f4d990c29e725df9b"
   },
   "source": [
    "## Preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "a1f8a5fb-54d5-495f-8e1e-9b481815de7f",
    "_uuid": "ccbf988974116587e0eac503b3dd88186aad3e19"
   },
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "3dd91627-ae7d-48fe-b081-e2770f1d18ce",
    "_uuid": "5fc66181bd30b44e6784818f01e0037fc1f5b997"
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../input/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "004d1ee7-4723-46c0-9b36-4c595e09bfec",
    "_uuid": "c7af12201683ca1cc996c0290e4e6b25f597bd53"
   },
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ' '.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result\n",
    "\n",
    "def preprocess(path_to_inp_json_file):\n",
    "    output_list = []\n",
    "    with open(path_to_inp_json_file, encoding='utf-8') as inp_file:\n",
    "        for line in tqdm_notebook(inp_file):\n",
    "            json_data = read_json_line(line)\n",
    "            content = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content_no_html_tags = strip_tags(content)\n",
    "            output_list.append(content_no_html_tags)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6404b5d2ff6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_raw_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_inp_json_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-6976b9927d9b>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(path_to_inp_json_file)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_inp_json_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moutput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_inp_json_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minp_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/train.json'"
     ]
    }
   ],
   "source": [
    "train_raw_content = preprocess(path_to_inp_json_file=os.path.join(PATH_TO_DATA, 'train.json'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "28d8627d-83d8-4d7a-9d3e-1f3d22eedc92",
    "_uuid": "c72ea1678db92eff5ae626c5ae9444636b7711fa"
   },
   "outputs": [],
   "source": [
    "test_raw_content = preprocess(path_to_inp_json_file=os.path.join(PATH_TO_DATA,  'test.json'),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff6e5e82-d14e-4ed7-a63d-0f0b75360bba",
    "_uuid": "bb2417acb399ed415dcf69f22965f7554ee78aa0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=10000, min_df = 0.1, max_df = 0.8)\n",
    "sparse_train = cv.fit_transform(train_raw_content)\n",
    "sparse_test  = cv.transform(test_raw_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f4ec219e-54fe-440b-9ba7-25e7d1491f88",
    "_uuid": "e15abdd0ae614905d3bb3cca4b90ae194db5171e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_sparse_data =  sparse.vstack([sparse_train, sparse_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ab07f4b0-3d93-4788-bc9a-3e4f2a94a9fa",
    "_uuid": "06f33fa79856df1c334fa5fb22b15d713a2e5e3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_log1p_recommends.csv'), \n",
    "                           index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "583868f4-dfd8-4a42-8b5f-97304108d596",
    "_uuid": "fe4072d867630c4a490ec5b108e70911c5ad18ee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "471388af-7b24-47a4-b023-440a2c93711a",
    "_uuid": "86578ef6fff5d5b334a397dd5ff613d41275565a"
   },
   "source": [
    "## Extracting topics with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "747cfece-6765-4fbb-a945-baaa51ccaac9",
    "_uuid": "70c0e0a0d2394068281fb780645b2aee2491630e"
   },
   "source": [
    "Shortly, LDA represents documents as mixtures of topics that spit out words with certain probabilities.\n",
    "\n",
    "For each possible topic Z, we'll multiply the frequency of this word type W in Z by the number of other words in document D that already belong to Z. The result will represent the probability that this word came from Z. Here's the actual formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42e27c89-ffd2-4141-b57e-be39da8b6d4b",
    "_uuid": "bcd68bda7bca96951c3aff06a9c75a187bf6de1d"
   },
   "source": [
    "![](http://tedunderwood.files.wordpress.com/2012/04/ldaformula.png?w=584****)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "585ce658-4123-4fb8-9c27-bf58fef8ed77",
    "_uuid": "22ec9744a61c8382fabe331f50ad0f0ea3460832"
   },
   "source": [
    "Finding the right parameters for LDA is 'an art'. \n",
    "\n",
    "3 main parameters need to be optimized:\n",
    "1. ** K**: the number of topics\n",
    "2. **Alpha** which dictates how many topics a document potentially has. The lower alpha, the lower the number of topics per documents\n",
    "3. **Beta** which dictates the number of word per document. Similarly to Alpha, the lower Beta is, the lower the number for words per topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5bcf60f3-2e8d-42c0-a829-80b029ea3db2",
    "_uuid": "3985256cebe27f347d7f43848c137a670d8e511a"
   },
   "source": [
    "That is all with theory, let is LDA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "67809283-25e6-4fda-b9f5-0b8ea0e566a2",
    "_uuid": "33ea20e05837c28e3b8e82df47ec6fb9e26d7f81"
   },
   "source": [
    "I'll use realisation of LDA from gensim library, so it needs some data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f13c96f6-60e6-4dbe-93cb-cc2ed30c951d",
    "_uuid": "6a284cdb8b2b3525c648643d8e9597db97302fc0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transform our sparse_data to corpus for gensim\n",
    "corpus_data_gensim = gensim.matutils.Sparse2Corpus(full_sparse_data, documents_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0752c526-4dfd-445b-8834-729337ac405a",
    "_uuid": "5b0b8e2db72d766ce71a67f81ea518664f79615a"
   },
   "outputs": [],
   "source": [
    "#Create dictionary for LDA model\n",
    "vocabulary_gensim = {}\n",
    "for key, val in cv.vocabulary_.items():\n",
    "    vocabulary_gensim[val] = key\n",
    "    \n",
    "dict = Dictionary()\n",
    "dict.merge_with(vocabulary_gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f7265a79-c6aa-4200-8e15-bdbbbb8e7659",
    "_uuid": "4db87a6b5c071fe90620fdd5e0dd0917b2e4cd68"
   },
   "source": [
    "Let's assume, that we can devide our articels in 30 different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5e86842e-502f-4a26-bf09-03fbb703d78c",
    "_uuid": "b18eb9b2a7b4f8f653403e8c63dd7275e86fc49e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus_data_gensim, num_topics = 30 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cac503f2-d053-4a1b-b4b6-825ecf1f3106",
    "_uuid": "dcad2cefb93857550f225bf2f142319ea8d41b1f"
   },
   "source": [
    "Let's look at our topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f51175e1-4c81-4639-8d7f-6a54afc65184",
    "_uuid": "6f20a2e2a16d5ec61f426f377a8e777be1eaa7fd"
   },
   "outputs": [],
   "source": [
    "data_ =  pyLDAvis.gensim.prepare(lda, corpus_data_gensim, dict)\n",
    "# pyLDAvis.display(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "927e4886-371a-4069-ba5b-6d29fc8aa43b",
    "_uuid": "75ac216b8d6c7078dd10552fe435a3adcc9b255b"
   },
   "source": [
    "I commented the code and inserted the image because pyLDAvis caused kernel disturtion, but you can download the notebook and run notebook by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "286c6fac-e0c3-4739-a352-620c5e5db133",
    "_uuid": "0aaf4a45b615ba322f7a7e9931aa22f6e6166a0d"
   },
   "source": [
    "![](http://github.com/Twoweaks/random/blob/master/lda.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aa593bd4-7fcf-4c7c-8672-e67a835e0a95",
    "_uuid": "9e4b843a2742783a492fee4cd43bd960b069a34a"
   },
   "source": [
    "Ð¡ircles represent different topics and the distance between them. Similar topics appear closer and the dissimilar topics farther. The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus. Despite our model was built with corpus of only 10000 words, we can understand the general tone of some topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc66e697-5fab-488e-8427-a1fd2c995628",
    "_uuid": "01fa75e584404c883ddf43c1517340321b8e1273"
   },
   "source": [
    "Obviously, we can definitely improve this to achieve better separation between the topics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e4599a21-401f-4696-a683-91edc2eae6ee",
    "_uuid": "aff50a1e123b51ddfd696dd60225deac7c17172c"
   },
   "source": [
    "Transforms a bag of words document to features. It returns the proportion of how much each topic was present in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cb0961b2-aae6-433e-9354-c128c3082aad",
    "_uuid": "8773889d5036d260de3b867fb4ddb28784d2f7e8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_to_lda_features(lda_model, document):\n",
    "    topic_importances = lda.get_document_topics(document, minimum_probability=0)\n",
    "    topic_importances = np.array(topic_importances)\n",
    "    return topic_importances[:,1]\n",
    "\n",
    "lda_features = list(map(lambda doc:document_to_lda_features(lda, doc),corpus_data_gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f6007be6-2b64-45f4-b976-d80fe3771575",
    "_uuid": "bd9283bf06f96bfc6b0a3002911d428b589e8fc1"
   },
   "outputs": [],
   "source": [
    "data_pd_lda_features = pd.DataFrame(lda_features)\n",
    "data_pd_lda_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "21e56367-7aae-4764-977e-15f6c0dbaeb2",
    "_uuid": "b228b8cb0d0819c0a30695d2b671c8af41bd0f01"
   },
   "source": [
    "Let's look at the correlation of generated lda features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1d28eea-cfae-48a9-a908-d781d4a6b9a0",
    "_uuid": "e4ee4b0b6a9d19c86974b5b5c4d5c10a3d756042"
   },
   "outputs": [],
   "source": [
    "data_pd_lda_features_train = data_pd_lda_features.iloc[:y_train.shape[0]]\n",
    "data_pd_lda_features_train['target'] = y_train\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# the size of A4 paper\n",
    "fig.set_size_inches(20.7, 8.27)\n",
    "sns.heatmap(data_pd_lda_features_train.corr(method = 'spearman'), cmap=\"RdYlGn\", ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e07db7d-7330-4499-a7cd-1283695830e6",
    "_uuid": "c79928354e4e313b9636e676de5c47715ab80302"
   },
   "source": [
    "Some topics have correlation with target variable (topic 14 or topic 15).\n",
    "\n",
    "So we can use probability of topics for each article  as  features for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a790c6d6-418d-4441-8271-0b03dd9f975d",
    "_uuid": "7a675074f474f139c9c833d2e4e07e45a887a4f0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr = sparse.hstack([sparse_train, data_pd_lda_features_train.drop('target', axis = 1)]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0ccd87d-661c-44ab-acf9-238fe06846ec",
    "_uuid": "03e1f1dc8129079ca07c1268169f45e88f5a78c7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = sparse.hstack([sparse_test, data_pd_lda_features.iloc[y_train.shape[0]:]]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e24b66ec-5639-4029-a102-93788d59cdb8",
    "_uuid": "ff78f589227d095920a956d584d88c3e7a1fb8d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(random_state=17)\n",
    "ridge.fit(X_tr,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ef4143e5-36e8-4330-81a8-9836e6f3c877",
    "_uuid": "fa39812ca277a79a8fa007cf251506aede031030",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5574ca3f-af7e-423b-85c7-19aa0bb445de",
    "_uuid": "ce53d842306c4f2ac8930fa7bd765e51ff548787",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(subm, bins=30, alpha=.5, color='green', label='pred', range=(0,10));\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "def79706-50ae-4db6-b874-8422df6432a2",
    "_uuid": "dd91b865f8695853d6a01c829a0d3542d18f6ee9"
   },
   "source": [
    "*What's next:*\n",
    "* in this kernel there was only some basic ideas of how to add some more features to your model, tunig LDA will boost your result on leaderboard\n",
    "* change number of features for Countvectorizer,  tune some parametrs of it\n",
    "* change LDA parametrs (more topics, find optimal beta and alpha)\n",
    "\n",
    "Good Luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
